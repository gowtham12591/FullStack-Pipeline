Full Stack Data Pipeline Project:

The idea of the project is to create an end-end pipeline, i,e; starting from data extraction till the deployment. For data pipeline ETL process has been followed.\

This project follows the below process,
1. The data extraction/creation has been done using faker library. The created data is pushed to MongoDB for data storage.
2. One more set of data is stored in MySQL db.
3. Both the data from MySQL and MongoDB is ingested using Pandas library.
4. The extracted dataset is merged and all the necessary preprocessing is done.
5. Visualisation reports are generated for the pre-processed data
6. Data version control is applied to the cleansed data after pre-processing.
7. Final cleansed data is stored/pushed to data warehouse for future use.
8. Training and validating the model using all the supervised learning algorithms
9. Freezing the best obtained model from the previous step
10. Applying MLOps using MLFlow (Model Registering and Model Serving)
11. Using the endpoint obtained from previous step, deploying the model using Flask API
12. Using Docker-Containers create an image of the entire project for future use.

Tools / Technology used:
Scripting Language: Python3.9.12
Platform: On-Prem
Backend: MySQL, MongoDB
Messaging Queue: Apache Kafka
Data Generator: Faker-Python
Data Version Control: Date-time library in python
Models: Linear Regression, SVR, DTR, RFR, GBR, XGBR, Stacking Regressor
Frontend: Tkinter
Framework: Flask
MLOps: MLFlow
Image: Docker
